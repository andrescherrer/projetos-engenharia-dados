# Passo a Passo: Pipeline com Processamento Distribu√≠do

## Objetivo
Implementar um pipeline usando Spark para processamento distribu√≠do de grandes volumes de dados, aprendendo quando e como usar processamento distribu√≠do.

## Pr√©-requisitos
- Python 3.8+
- Java 8+ (requerido pelo Spark)
- Conhecimento b√°sico de Spark/PySpark

## Passo 1: Quando Usar Processamento Distribu√≠do

**Use Spark quando:**
- Dados n√£o cabem na mem√≥ria de uma m√°quina
- Processamento leva muito tempo (>30min)
- Precisa processar terabytes de dados
- Precisa de paralelismo massivo

**Use Pandas quando:**
- Dados cabem na mem√≥ria (<10GB)
- Processamento √© r√°pido (<5min)
- L√≥gica √© complexa e dif√≠cil de paralelizar

## Passo 2: Configurar Spark Local

```bash
# Instalar PySpark
pip install pyspark

# Verificar instala√ß√£o
python -c "from pyspark.sql import SparkSession; print('Spark OK')"
```

## Passo 3: Estrutura do Projeto

```
spark-pipeline/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ extract/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ spark_extract.py
‚îÇ   ‚îú‚îÄ‚îÄ transform/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ spark_transform.py
‚îÇ   ‚îú‚îÄ‚îÄ load/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ spark_load.py
‚îÇ   ‚îî‚îÄ‚îÄ utils/
‚îÇ       ‚îî‚îÄ‚îÄ spark_config.py
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îî‚îÄ‚îÄ input/
‚îî‚îÄ‚îÄ notebooks/
    ‚îî‚îÄ‚îÄ spark_analysis.ipynb
```

## Passo 4: Configurar Spark Session

**src/utils/spark_config.py:**
```python
from pyspark.sql import SparkSession
from pyspark import SparkConf

def create_spark_session(
    app_name: str = "Data Pipeline",
    master: str = "local[*]",
    memory: str = "4g"
) -> SparkSession:
    """Cria SparkSession otimizada"""
    
    conf = SparkConf().setAppName(app_name) \
        .setMaster(master) \
        .set("spark.executor.memory", memory) \
        .set("spark.driver.memory", memory) \
        .set("spark.sql.shuffle.partitions", "200") \
        .set("spark.default.parallelism", "200") \
        .set("spark.sql.adaptive.enabled", "true") \
        .set("spark.sql.adaptive.coalescePartitions.enabled", "true")
    
    spark = SparkSession.builder \
        .config(conf=conf) \
        .getOrCreate()
    
    spark.sparkContext.setLogLevel("WARN")
    
    return spark
```

## Passo 5: Extra√ß√£o com Spark

**src/extract/spark_extract.py:**
```python
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, input_file_name, current_timestamp

class SparkExtractor:
    """Extrai dados usando Spark"""
    
    def __init__(self, spark: SparkSession):
        self.spark = spark
    
    def extract_csv(self, path: str, header: bool = True):
        """Extrai CSV distribu√≠do"""
        print(f"üîÑ Extraindo CSV: {path}")
        
        df = self.spark.read \
            .option("header", str(header).lower()) \
            .option("inferSchema", "true") \
            .csv(path)
        
        # Adicionar metadados
        df = df.withColumn("source_file", input_file_name()) \
               .withColumn("extracted_at", current_timestamp())
        
        print(f"‚úÖ Extra√≠do: {df.count()} registros, {len(df.columns)} colunas")
        return df
    
    def extract_parquet(self, path: str):
        """Extrai Parquet (formato otimizado)"""
        print(f"üîÑ Extraindo Parquet: {path}")
        
        df = self.spark.read.parquet(path)
        print(f"‚úÖ Extra√≠do: {df.count()} registros")
        return df
    
    def extract_json(self, path: str, multiline: bool = True):
        """Extrai JSON"""
        print(f"üîÑ Extraindo JSON: {path}")
        
        df = self.spark.read \
            .option("multiline", str(multiline).lower()) \
            .json(path)
        
        print(f"‚úÖ Extra√≠do: {df.count()} registros")
        return df
    
    def extract_from_database(self, jdbc_url: str, table: str, 
                             user: str, password: str):
        """Extrai de banco de dados"""
        print(f"üîÑ Extraindo do banco: {table}")
        
        df = self.spark.read \
            .format("jdbc") \
            .option("url", jdbc_url) \
            .option("dbtable", table) \
            .option("user", user) \
            .option("password", password) \
            .option("numPartitions", "10") \
            .option("partitionColumn", "id") \
            .option("lowerBound", "1") \
            .option("upperBound", "1000000") \
            .load()
        
        print(f"‚úÖ Extra√≠do: {df.count()} registros")
        return df
```

## Passo 6: Transforma√ß√µes com Spark

**src/transform/spark_transform.py:**
```python
from pyspark.sql import SparkSession, DataFrame
from pyspark.sql.functions import (
    col, when, sum, avg, count, max, min,
    date_format, year, month, dayofmonth,
    regexp_replace, trim, upper, lower
)
from pyspark.sql.window import Window

class SparkTransformer:
    """Aplica transforma√ß√µes distribu√≠das"""
    
    def __init__(self, spark: SparkSession):
        self.spark = spark
    
    def clean_data(self, df: DataFrame) -> DataFrame:
        """Limpa dados"""
        print("üîÑ Limpando dados...")
        
        # Remover duplicatas
        df_clean = df.dropDuplicates()
        
        # Trim strings
        for col_name in df_clean.columns:
            if df_clean.schema[col_name].dataType.typeName() == 'string':
                df_clean = df_clean.withColumn(
                    col_name,
                    trim(col(col_name))
                )
        
        print(f"‚úÖ Dados limpos: {df_clean.count()} registros")
        return df_clean
    
    def aggregate_data(
        self,
        df: DataFrame,
        group_by_cols: list,
        agg_dict: dict
    ) -> DataFrame:
        """Agrega dados"""
        print(f"üîÑ Agregando por: {group_by_cols}")
        
        # Construir express√µes de agrega√ß√£o
        agg_exprs = []
        for col_name, func in agg_dict.items():
            if func == 'sum':
                agg_exprs.append(sum(col(col_name)).alias(f"total_{col_name}"))
            elif func == 'avg':
                agg_exprs.append(avg(col(col_name)).alias(f"avg_{col_name}"))
            elif func == 'count':
                agg_exprs.append(count(col(col_name)).alias(f"count_{col_name}"))
            elif func == 'max':
                agg_exprs.append(max(col(col_name)).alias(f"max_{col_name}"))
            elif func == 'min':
                agg_exprs.append(min(col(col_name)).alias(f"min_{col_name}"))
        
        df_agg = df.groupBy(*group_by_cols).agg(*agg_exprs)
        
        print(f"‚úÖ Agrega√ß√£o conclu√≠da: {df_agg.count()} grupos")
        return df_agg
    
    def join_dataframes(
        self,
        df1: DataFrame,
        df2: DataFrame,
        join_key: str,
        join_type: str = "inner"
    ) -> DataFrame:
        """Faz join de DataFrames"""
        print(f"üîÑ Fazendo join {join_type} por {join_key}")
        
        df_joined = df1.join(df2, on=join_key, how=join_type)
        
        print(f"‚úÖ Join conclu√≠do: {df_joined.count()} registros")
        return df_joined
    
    def window_aggregation(
        self,
        df: DataFrame,
        partition_by: list,
        order_by: str,
        window_func: str = "sum",
        value_col: str = "valor"
    ) -> DataFrame:
        """Aplica fun√ß√µes de janela"""
        print(f"üîÑ Aplicando window function")
        
        window_spec = Window.partitionBy(*partition_by).orderBy(order_by)
        
        if window_func == "sum":
            df_windowed = df.withColumn(
                f"{window_func}_{value_col}",
                sum(col(value_col)).over(window_spec)
            )
        elif window_func == "avg":
            df_windowed = df.withColumn(
                f"{window_func}_{value_col}",
                avg(col(value_col)).over(window_spec)
            )
        
        print(f"‚úÖ Window function aplicada")
        return df_windowed
    
    def cache_dataframe(self, df: DataFrame, cache_type: str = "MEMORY"):
        """Cacheia DataFrame para reutiliza√ß√£o"""
        if cache_type == "MEMORY":
            df.cache()
        elif cache_type == "DISK":
            df.persist(StorageLevel.DISK_ONLY)
        elif cache_type == "MEMORY_AND_DISK":
            df.persist(StorageLevel.MEMORY_AND_DISK)
        
        print(f"‚úÖ DataFrame cacheado: {cache_type}")
        return df
```

## Passo 7: Otimiza√ß√µes de Performance

**src/utils/optimize.py:**
```python
from pyspark.sql import SparkSession, DataFrame

class SparkOptimizer:
    """Otimiza√ß√µes para Spark"""
    
    @staticmethod
    def repartition_by_column(df: DataFrame, column: str, num_partitions: int = None):
        """Reparticiona por coluna (melhora joins e agrega√ß√µes)"""
        if num_partitions:
            return df.repartition(num_partitions, column)
        else:
            return df.repartition(column)
    
    @staticmethod
    def coalesce_partitions(df: DataFrame, num_partitions: int):
        """Reduz n√∫mero de parti√ß√µes (√∫til ap√≥s filtros)"""
        return df.coalesce(num_partitions)
    
    @staticmethod
    def broadcast_join(df_small: DataFrame, df_large: DataFrame, join_key: str):
        """Faz broadcast join para tabelas pequenas"""
        from pyspark.sql.functions import broadcast
        
        return df_large.join(broadcast(df_small), on=join_key)
    
    @staticmethod
    def optimize_joins(df1: DataFrame, df2: DataFrame, join_key: str):
        """Otimiza joins"""
        # Se uma tabela √© pequena (<100MB), usar broadcast
        # Caso contr√°rio, garantir que ambas est√£o particionadas pela join key
        
        # Exemplo: verificar tamanho (simplificado)
        # Em produ√ß√£o, usar estimativas do Spark
        
        return df1.join(df2, on=join_key)
```

## Passo 8: Pipeline Completo

**main.py:**
```python
from src.utils.spark_config import create_spark_session
from src.extract.spark_extract import SparkExtractor
from src.transform.spark_transform import SparkTransformer
from src.load.spark_load import SparkLoader

def main():
    # Criar Spark Session
    spark = create_spark_session("Data Pipeline", memory="8g")
    
    print("=" * 60)
    print("PIPELINE COM PROCESSAMENTO DISTRIBU√çDO")
    print("=" * 60)
    
    # Extract
    extractor = SparkExtractor(spark)
    df_vendas = extractor.extract_csv("data/input/vendas.csv")
    df_produtos = extractor.extract_csv("data/input/produtos.csv")
    
    # Transform
    transformer = SparkTransformer(spark)
    
    # Limpar dados
    df_vendas_clean = transformer.clean_data(df_vendas)
    
    # Cache para reutiliza√ß√£o
    df_vendas_clean.cache()
    
    # Join com produtos
    df_joined = transformer.join_dataframes(
        df_vendas_clean,
        df_produtos,
        "produto_id"
    )
    
    # Agrega√ß√µes
    df_agg = transformer.aggregate_data(
        df_joined,
        group_by_cols=["categoria", "mes"],
        agg_dict={
            "valor": "sum",
            "quantidade": "sum",
            "valor": "avg"
        }
    )
    
    # Load
    loader = SparkLoader(spark)
    loader.save_parquet(df_agg, "data/output/agregacoes")
    
    # Limpar cache
    df_vendas_clean.unpersist()
    
    print("\n‚úÖ Pipeline conclu√≠do!")
    spark.stop()

if __name__ == "__main__":
    main()
```

## Passo 9: Monitoramento e Debugging

**src/utils/monitor.py:**
```python
from pyspark.sql import SparkSession

class SparkMonitor:
    """Monitora execu√ß√£o do Spark"""
    
    def __init__(self, spark: SparkSession):
        self.spark = spark
    
    def get_stage_info(self):
        """Obt√©m informa√ß√µes dos stages"""
        status_tracker = self.spark.sparkContext.statusTracker()
        stage_infos = status_tracker.getActiveStageInfos()
        
        for stage_info in stage_infos:
            print(f"Stage {stage_info.stageId}: {stage_info.numTasks} tasks")
    
    def explain_plan(self, df):
        """Explica plano de execu√ß√£o"""
        df.explain(extended=True)
    
    def get_partition_info(self, df):
        """Obt√©m informa√ß√µes de parti√ß√µes"""
        rdd = df.rdd
        print(f"N√∫mero de parti√ß√µes: {rdd.getNumPartitions()}")
        print(f"Tamanho estimado: {rdd.count()} registros")
```

## Passo 10: Compara√ß√£o Pandas vs Spark

**comparison.py:**
```python
import pandas as pd
from pyspark.sql import SparkSession

def pandas_approach(csv_path: str):
    """Abordagem com Pandas"""
    import time
    start = time.time()
    
    df = pd.read_csv(csv_path)
    result = df.groupby('categoria').agg({
        'valor': 'sum',
        'quantidade': 'sum'
    })
    
    elapsed = time.time() - start
    print(f"Pandas: {elapsed:.2f}s - {len(result)} grupos")
    return result

def spark_approach(csv_path: str):
    """Abordagem com Spark"""
    import time
    start = time.time()
    
    spark = SparkSession.builder.appName("Comparison").getOrCreate()
    df = spark.read.csv(csv_path, header=True, inferSchema=True)
    
    from pyspark.sql.functions import sum as spark_sum
    result = df.groupBy("categoria").agg(
        spark_sum("valor").alias("total_valor"),
        spark_sum("quantidade").alias("total_quantidade")
    )
    
    result_count = result.count()
    elapsed = time.time() - start
    
    print(f"Spark: {elapsed:.2f}s - {result_count} grupos")
    spark.stop()
    return result

# Testar com diferentes tamanhos de dados
# Para dados pequenos (<1GB): Pandas √© mais r√°pido
# Para dados grandes (>10GB): Spark √© necess√°rio
```

## Checklist de Conclus√£o

- [ ] Spark configurado
- [ ] Extra√ß√£o distribu√≠da implementada
- [ ] Transforma√ß√µes otimizadas
- [ ] Joins eficientes
- [ ] Cache implementado
- [ ] Particionamento otimizado
- [ ] Pipeline completo funcionando
- [ ] Performance monitorada
- [ ] Documenta√ß√£o completa
